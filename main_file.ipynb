{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oz46zfB9efSf"
      },
      "source": [
        "# Check GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5p5qN1Fl6zdL",
        "outputId": "f2e98204-84c5-47ec-bc12-00293e574a0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2yICeIpd4Md"
      },
      "source": [
        "# Installing the Repo and the Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-zHfNwW56-_w",
        "outputId": "c1c4b020-0d28-4bd8-9552-5b2997cd26c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'privateGPT'...\n",
            "remote: Enumerating objects: 358, done.\u001b[K\n",
            "remote: Total 358 (delta 0), reused 0 (delta 0), pack-reused 358\u001b[K\n",
            "Receiving objects: 100% (358/358), 207.53 KiB | 1.06 MiB/s, done.\n",
            "Resolving deltas: 100% (186/186), done.\n",
            "Collecting langchain==0.0.228 (from -r /content/privateGPT/requirements.txt (line 1))\n",
            "  Downloading langchain-0.0.228-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gpt4all==1.0.3 (from -r /content/privateGPT/requirements.txt (line 2))\n",
            "  Downloading gpt4all-1.0.3-py3-none-manylinux1_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting chromadb==0.3.26 (from -r /content/privateGPT/requirements.txt (line 3))\n",
            "  Downloading chromadb-0.3.26-py3-none-any.whl (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.6/123.6 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting llama-cpp-python==0.1.68 (from -r /content/privateGPT/requirements.txt (line 4))\n",
            "  Downloading llama_cpp_python-0.1.68.tar.gz (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting urllib3==2.0.3 (from -r /content/privateGPT/requirements.txt (line 5))\n",
            "  Downloading urllib3-2.0.3-py3-none-any.whl (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.6/123.6 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting PyMuPDF==1.22.5 (from -r /content/privateGPT/requirements.txt (line 6))\n",
            "  Downloading PyMuPDF-1.22.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv==1.0.0 (from -r /content/privateGPT/requirements.txt (line 7))\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Collecting unstructured==0.8.0 (from -r /content/privateGPT/requirements.txt (line 8))\n",
            "  Downloading unstructured-0.8.0-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting extract-msg==0.41.5 (from -r /content/privateGPT/requirements.txt (line 9))\n",
            "  Downloading extract_msg-0.41.5-py2.py3-none-any.whl (185 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m185.2/185.2 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tabulate==0.9.0 in /usr/local/lib/python3.10/dist-packages (from -r /content/privateGPT/requirements.txt (line 10)) (0.9.0)\n",
            "Collecting pandoc==2.3 (from -r /content/privateGPT/requirements.txt (line 11))\n",
            "  Downloading pandoc-2.3.tar.gz (33 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pypandoc==1.11 (from -r /content/privateGPT/requirements.txt (line 12))\n",
            "  Downloading pypandoc-1.11-py3-none-any.whl (20 kB)\n",
            "Collecting tqdm==4.65.0 (from -r /content/privateGPT/requirements.txt (line 13))\n",
            "  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.1/77.1 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.228->-r /content/privateGPT/requirements.txt (line 1)) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.228->-r /content/privateGPT/requirements.txt (line 1)) (2.0.27)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.228->-r /content/privateGPT/requirements.txt (line 1)) (3.9.3)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.228->-r /content/privateGPT/requirements.txt (line 1)) (4.0.3)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain==0.0.228->-r /content/privateGPT/requirements.txt (line 1))\n",
            "  Downloading dataclasses_json-0.5.14-py3-none-any.whl (26 kB)\n",
            "Collecting langchainplus-sdk<0.0.21,>=0.0.20 (from langchain==0.0.228->-r /content/privateGPT/requirements.txt (line 1))\n",
            "  Downloading langchainplus_sdk-0.0.20-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.228->-r /content/privateGPT/requirements.txt (line 1)) (2.9.0)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.228->-r /content/privateGPT/requirements.txt (line 1)) (1.25.2)\n",
            "Collecting openapi-schema-pydantic<2.0,>=1.2 (from langchain==0.0.228->-r /content/privateGPT/requirements.txt (line 1))\n",
            "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydantic<2,>=1 (from langchain==0.0.228->-r /content/privateGPT/requirements.txt (line 1))\n",
            "  Downloading pydantic-1.10.14-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m66.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.228->-r /content/privateGPT/requirements.txt (line 1)) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.228->-r /content/privateGPT/requirements.txt (line 1)) (8.2.3)\n",
            "Requirement already satisfied: pandas>=1.3 in /usr/local/lib/python3.10/dist-packages (from chromadb==0.3.26->-r /content/privateGPT/requirements.txt (line 3)) (1.5.3)\n",
            "Collecting hnswlib>=0.7 (from chromadb==0.3.26->-r /content/privateGPT/requirements.txt (line 3))\n",
            "  Downloading hnswlib-0.8.0.tar.gz (36 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting clickhouse-connect>=0.5.7 (from chromadb==0.3.26->-r /content/privateGPT/requirements.txt (line 3))\n",
            "  Downloading clickhouse_connect-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (964 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m964.5/964.5 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: duckdb>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from chromadb==0.3.26->-r /content/privateGPT/requirements.txt (line 3)) (0.9.2)\n",
            "Collecting fastapi>=0.85.1 (from chromadb==0.3.26->-r /content/privateGPT/requirements.txt (line 3))\n",
            "  Downloading fastapi-0.110.0-py3-none-any.whl (92 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.1/92.1 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn[standard]>=0.18.3 (from chromadb==0.3.26->-r /content/privateGPT/requirements.txt (line 3))\n",
            "  Downloading uvicorn-0.27.1-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting posthog>=2.4.0 (from chromadb==0.3.26->-r /content/privateGPT/requirements.txt (line 3))\n",
            "  Downloading posthog-3.4.2-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb==0.3.26->-r /content/privateGPT/requirements.txt (line 3)) (4.9.0)\n",
            "Collecting pulsar-client>=3.1.0 (from chromadb==0.3.26->-r /content/privateGPT/requirements.txt (line 3))\n",
            "  Downloading pulsar_client-3.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m55.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnxruntime>=1.14.1 (from chromadb==0.3.26->-r /content/privateGPT/requirements.txt (line 3))\n",
            "  Downloading onnxruntime-1.17.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m92.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb==0.3.26->-r /content/privateGPT/requirements.txt (line 3)) (0.15.2)\n",
            "Collecting overrides>=7.3.1 (from chromadb==0.3.26->-r /content/privateGPT/requirements.txt (line 3))\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python==0.1.68->-r /content/privateGPT/requirements.txt (line 4))\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting argilla (from unstructured==0.8.0->-r /content/privateGPT/requirements.txt (line 8))\n",
            "  Downloading argilla-1.25.0-py3-none-any.whl (415 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m415.1/415.1 kB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from unstructured==0.8.0->-r /content/privateGPT/requirements.txt (line 8)) (5.2.0)\n",
            "Collecting filetype (from unstructured==0.8.0->-r /content/privateGPT/requirements.txt (line 8))\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from unstructured==0.8.0->-r /content/privateGPT/requirements.txt (line 8)) (4.9.4)\n",
            "Collecting msg-parser (from unstructured==0.8.0->-r /content/privateGPT/requirements.txt (line 8))\n",
            "  Downloading msg_parser-1.2.0-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.8/101.8 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from unstructured==0.8.0->-r /content/privateGPT/requirements.txt (line 8)) (3.8.1)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (from unstructured==0.8.0->-r /content/privateGPT/requirements.txt (line 8)) (3.1.2)\n",
            "Collecting pdf2image (from unstructured==0.8.0->-r /content/privateGPT/requirements.txt (line 8))\n",
            "  Downloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
            "Collecting pdfminer.six (from unstructured==0.8.0->-r /content/privateGPT/requirements.txt (line 8))\n",
            "  Downloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m81.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from unstructured==0.8.0->-r /content/privateGPT/requirements.txt (line 8)) (9.4.0)\n",
            "Collecting python-docx (from unstructured==0.8.0->-r /content/privateGPT/requirements.txt (line 8))\n",
            "  Downloading python_docx-1.1.0-py3-none-any.whl (239 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.6/239.6 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-pptx (from unstructured==0.8.0->-r /content/privateGPT/requirements.txt (line 8))\n",
            "  Downloading python_pptx-0.6.23-py3-none-any.whl (471 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-magic (from unstructured==0.8.0->-r /content/privateGPT/requirements.txt (line 8))\n",
            "  Downloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: markdown in /usr/local/lib/python3.10/dist-packages (from unstructured==0.8.0->-r /content/privateGPT/requirements.txt (line 8)) (3.5.2)\n",
            "Requirement already satisfied: xlrd in /usr/local/lib/python3.10/dist-packages (from unstructured==0.8.0->-r /content/privateGPT/requirements.txt (line 8)) (2.0.1)\n",
            "Collecting imapclient<3,>=2.3.0 (from extract-msg==0.41.5->-r /content/privateGPT/requirements.txt (line 9))\n",
            "  Downloading IMAPClient-2.3.1-py2.py3-none-any.whl (181 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting olefile==0.46 (from extract-msg==0.41.5->-r /content/privateGPT/requirements.txt (line 9))\n",
            "  Downloading olefile-0.46.zip (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.2/112.2 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tzlocal<6,>=4.2 in /usr/local/lib/python3.10/dist-packages (from extract-msg==0.41.5->-r /content/privateGPT/requirements.txt (line 9)) (5.2)\n",
            "Collecting compressed-rtf<2,>=1.0.6 (from extract-msg==0.41.5->-r /content/privateGPT/requirements.txt (line 9))\n",
            "  Downloading compressed_rtf-1.0.6.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ebcdic<2,>=1.1.1 (from extract-msg==0.41.5->-r /content/privateGPT/requirements.txt (line 9))\n",
            "  Downloading ebcdic-1.1.1-py2.py3-none-any.whl (128 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.5/128.5 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4<4.13,>=4.11.1 in /usr/local/lib/python3.10/dist-packages (from extract-msg==0.41.5->-r /content/privateGPT/requirements.txt (line 9)) (4.12.3)\n",
            "Collecting RTFDE==0.0.2 (from extract-msg==0.41.5->-r /content/privateGPT/requirements.txt (line 9))\n",
            "  Downloading RTFDE-0.0.2-py3-none-any.whl (34 kB)\n",
            "Collecting red-black-tree-mod==1.20 (from extract-msg==0.41.5->-r /content/privateGPT/requirements.txt (line 9))\n",
            "  Downloading red-black-tree-mod-1.20.tar.gz (28 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting plumbum (from pandoc==2.3->-r /content/privateGPT/requirements.txt (line 11))\n",
            "  Downloading plumbum-1.8.2-py3-none-any.whl (127 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.0/127.0 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ply (from pandoc==2.3->-r /content/privateGPT/requirements.txt (line 11))\n",
            "  Downloading ply-3.11-py2.py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lark-parser>=0.11 (from RTFDE==0.0.2->extract-msg==0.41.5->-r /content/privateGPT/requirements.txt (line 9))\n",
            "  Downloading lark_parser-0.12.0-py2.py3-none-any.whl (103 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.5/103.5 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting oletools>=0.56 (from RTFDE==0.0.2->extract-msg==0.41.5->-r /content/privateGPT/requirements.txt (line 9))\n",
            "  Downloading oletools-0.60.1-py2.py3-none-any.whl (977 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m977.2/977.2 kB\u001b[0m \u001b[31m66.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.228->-r /content/privateGPT/requirements.txt (line 1)) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.228->-r /content/privateGPT/requirements.txt (line 1)) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.228->-r /content/privateGPT/requirements.txt (line 1)) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.228->-r /content/privateGPT/requirements.txt (line 1)) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.228->-r /content/privateGPT/requirements.txt (line 1)) (1.9.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<4.13,>=4.11.1->extract-msg==0.41.5->-r /content/privateGPT/requirements.txt (line 9)) (2.5)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from clickhouse-connect>=0.5.7->chromadb==0.3.26->-r /content/privateGPT/requirements.txt (line 3)) (2024.2.2)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from clickhouse-connect>=0.5.7->chromadb==0.3.26->-r /content/privateGPT/requirements.txt (line 3)) (2023.4)\n",
            "Collecting zstandard (from clickhouse-connect>=0.5.7->chromadb==0.3.26->-r /content/privateGPT/requirements.txt (line 3))\n",
            "  Downloading zstandard-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m86.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lz4 (from clickhouse-connect>=0.5.7->chromadb==0.3.26->-r /content/privateGPT/requirements.txt (line 3))\n",
            "  Downloading lz4-4.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m65.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.228->-r /content/privateGPT/requirements.txt (line 1))\n",
            "  Downloading marshmallow-3.21.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.228->-r /content/privateGPT/requirements.txt (line 1))\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting starlette<0.37.0,>=0.36.3 (from fastapi>=0.85.1->chromadb==0.3.26->-r /content/privateGPT/requirements.txt (line 3))\n",
            "  Downloading starlette-0.36.3-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from imapclient<3,>=2.3.0->extract-msg==0.41.5->-r /content/privateGPT/requirements.txt (line 9)) (1.16.0)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb==0.3.26->-r /content/privateGPT/requirements.txt (line 3))\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb==0.3.26->-r /content/privateGPT/requirements.txt (line 3)) (23.5.26)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb==0.3.26->-r /content/privateGPT/requirements.txt (line 3)) (23.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb==0.3.26->-r /content/privateGPT/requirements.txt (line 3)) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb==0.3.26->-r /content/privateGPT/requirements.txt (line 3)) (1.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3->chromadb==0.3.26->-r /content/privateGPT/requirements.txt (line 3)) (2.8.2)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb==0.3.26->-r /content/privateGPT/requirements.txt (line 3))\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb==0.3.26->-r /content/privateGPT/requirements.txt (line 3))\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.228->-r /content/privateGPT/requirements.txt (line 1)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.228->-r /content/privateGPT/requirements.txt (line 1)) (3.6)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.228->-r /content/privateGPT/requirements.txt (line 1)) (3.0.3)\n",
            "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.2->chromadb==0.3.26->-r /content/privateGPT/requirements.txt (line 3)) (0.20.3)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb==0.3.26->-r /content/privateGPT/requirements.txt (line 3)) (8.1.7)\n",
            "Collecting h11>=0.8 (from uvicorn[standard]>=0.18.3->chromadb==0.3.26->-r /content/privateGPT/requirements.txt (line 3))\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb==0.3.26->-r /content/privateGPT/requirements.txt (line 3))\n",
            "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb==0.3.26->-r /content/privateGPT/requirements.txt (line 3))\n",
            "  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m82.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb==0.3.26->-r /content/privateGPT/requirements.txt (line 3))\n",
            "  Downloading watchfiles-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb==0.3.26->-r /content/privateGPT/requirements.txt (line 3))\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx<=0.26,>=0.15 (from argilla->unstructured==0.8.0->-r /content/privateGPT/requirements.txt (line 8))\n",
            "  Downloading httpx-0.26.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting deprecated~=1.2.0 (from argilla->unstructured==0.8.0->-r /content/privateGPT/requirements.txt (line 8))\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.13 in /usr/local/lib/python3.10/dist-packages (from argilla->unstructured==0.8.0->-r /content/privateGPT/requirements.txt (line 8)) (1.14.1)\n",
            "Collecting numpy<2,>=1 (from langchain==0.0.228->-r /content/privateGPT/requirements.txt (line 1))\n",
            "  Downloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: rich!=13.1.0 in /usr/local/lib/python3.10/dist-packages (from argilla->unstructured==0.8.0->-r /content/privateGPT/requirements.txt (line 8)) (13.7.0)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from argilla->unstructured==0.8.0->-r /content/privateGPT/requirements.txt (line 8)) (0.9.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured==0.8.0->-r /content/privateGPT/requirements.txt (line 8)) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured==0.8.0->-r /content/privateGPT/requirements.txt (line 8)) (2023.12.25)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl->unstructured==0.8.0->-r /content/privateGPT/requirements.txt (line 8)) (1.1.0)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six->unstructured==0.8.0->-r /content/privateGPT/requirements.txt (line 8)) (42.0.3)\n",
            "Collecting XlsxWriter>=0.5.7 (from python-pptx->unstructured==0.8.0->-r /content/privateGPT/requirements.txt (line 8))\n",
            "  Downloading XlsxWriter-3.2.0-py3-none-any.whl (159 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.9/159.9 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six->unstructured==0.8.0->-r /content/privateGPT/requirements.txt (line 8)) (1.16.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<=0.26,>=0.15->argilla->unstructured==0.8.0->-r /content/privateGPT/requirements.txt (line 8)) (3.7.1)\n",
            "Collecting httpcore==1.* (from httpx<=0.26,>=0.15->argilla->unstructured==0.8.0->-r /content/privateGPT/requirements.txt (line 8))\n",
            "  Downloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<=0.26,>=0.15->argilla->unstructured==0.8.0->-r /content/privateGPT/requirements.txt (line 8)) (1.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb==0.3.26->-r /content/privateGPT/requirements.txt (line 3)) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb==0.3.26->-r /content/privateGPT/requirements.txt (line 3)) (2023.6.0)\n",
            "Collecting pyparsing<3,>=2.1.0 (from oletools>=0.56->RTFDE==0.0.2->extract-msg==0.41.5->-r /content/privateGPT/requirements.txt (line 9))\n",
            "  Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.8/67.8 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting easygui (from oletools>=0.56->RTFDE==0.0.2->extract-msg==0.41.5->-r /content/privateGPT/requirements.txt (line 9))\n",
            "  Downloading easygui-0.98.3-py2.py3-none-any.whl (92 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.7/92.7 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting colorclass (from oletools>=0.56->RTFDE==0.0.2->extract-msg==0.41.5->-r /content/privateGPT/requirements.txt (line 9))\n",
            "  Downloading colorclass-2.2.2-py2.py3-none-any.whl (18 kB)\n",
            "Collecting pcodedmp>=1.2.5 (from oletools>=0.56->RTFDE==0.0.2->extract-msg==0.41.5->-r /content/privateGPT/requirements.txt (line 9))\n",
            "  Downloading pcodedmp-1.2.6-py2.py3-none-any.whl (30 kB)\n",
            "Collecting msoffcrypto-tool (from oletools>=0.56->RTFDE==0.0.2->extract-msg==0.41.5->-r /content/privateGPT/requirements.txt (line 9))\n",
            "  Downloading msoffcrypto_tool-5.3.1-py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.4/46.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich!=13.1.0->argilla->unstructured==0.8.0->-r /content/privateGPT/requirements.txt (line 8)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich!=13.1.0->argilla->unstructured==0.8.0->-r /content/privateGPT/requirements.txt (line 8)) (2.16.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.228->-r /content/privateGPT/requirements.txt (line 1))\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb==0.3.26->-r /content/privateGPT/requirements.txt (line 3))\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb==0.3.26->-r /content/privateGPT/requirements.txt (line 3)) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<=0.26,>=0.15->argilla->unstructured==0.8.0->-r /content/privateGPT/requirements.txt (line 8)) (1.2.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six->unstructured==0.8.0->-r /content/privateGPT/requirements.txt (line 8)) (2.21)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich!=13.1.0->argilla->unstructured==0.8.0->-r /content/privateGPT/requirements.txt (line 8)) (0.1.2)\n",
            "Building wheels for collected packages: llama-cpp-python, pandoc, olefile, red-black-tree-mod, compressed-rtf, hnswlib\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.1.68-cp310-cp310-linux_x86_64.whl size=263458 sha256=598641324a9755b5e9d93d36f7c0318dc2f34371d8a24055b6c845baad443678\n",
            "  Stored in directory: /root/.cache/pip/wheels/df/f2/fb/b8153a244ace60fa4759cbd3d4881a2132b71e0e894ed6f29b\n",
            "  Building wheel for pandoc (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pandoc: filename=pandoc-2.3-py3-none-any.whl size=33261 sha256=287be92b0e1e3aeb2fdf74cc6afb8567e668db7c4bb564a4eafab08a9d22bbdf\n",
            "  Stored in directory: /root/.cache/pip/wheels/76/27/c2/c26175310aadcb8741b77657a1bb49c50cc7d4cdbf9eee0005\n",
            "  Building wheel for olefile (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for olefile: filename=olefile-0.46-py2.py3-none-any.whl size=35416 sha256=d4237423a3360cf1cca74f687dfdc7b558602dbd6d178c8a044dce8a1eddeb6b\n",
            "  Stored in directory: /root/.cache/pip/wheels/02/39/c0/9eb1f7a42b4b38f6f333b6314d4ed11c46f12a0f7b78194f0d\n",
            "  Building wheel for red-black-tree-mod (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for red-black-tree-mod: filename=red_black_tree_mod-1.20-py3-none-any.whl size=18620 sha256=e2b204f145cd815b3aa864661f0986dc3dd6f5a17414ffc765c238d8a1579afe\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/89/a0/17d08e78a59e4e8f51a95fe52e19c6916450c143acc7bce4dd\n",
            "  Building wheel for compressed-rtf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for compressed-rtf: filename=compressed_rtf-1.0.6-py3-none-any.whl size=6184 sha256=9301a9713a233f0d8699647bf158b1cb8d831d6a9874583d90741148c7a8b52d\n",
            "  Stored in directory: /root/.cache/pip/wheels/15/3e/48/e7d833ecc516c36f8966d310b1a6386db091a718f1ff3bf85c\n",
            "  Building wheel for hnswlib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hnswlib: filename=hnswlib-0.8.0-cp310-cp310-linux_x86_64.whl size=2287615 sha256=7b4e1fba585616d5b879070f2f35ba40b0cc9b6f0e64db852c95ecd47156034c\n",
            "  Stored in directory: /root/.cache/pip/wheels/af/a9/3e/3e5d59ee41664eb31a4e6de67d1846f86d16d93c45f277c4e7\n",
            "Successfully built llama-cpp-python pandoc olefile red-black-tree-mod compressed-rtf hnswlib\n",
            "Installing collected packages: red-black-tree-mod, ply, monotonic, lark-parser, filetype, ebcdic, easygui, compressed-rtf, zstandard, XlsxWriter, websockets, uvloop, urllib3, tqdm, python-magic, python-dotenv, python-docx, pyparsing, pypandoc, PyMuPDF, pydantic, pulsar-client, plumbum, pdf2image, overrides, olefile, numpy, mypy-extensions, marshmallow, lz4, imapclient, humanfriendly, httptools, h11, diskcache, deprecated, colorclass, backoff, watchfiles, uvicorn, typing-inspect, starlette, python-pptx, pandoc, openapi-schema-pydantic, msg-parser, llama-cpp-python, httpcore, hnswlib, coloredlogs, clickhouse-connect, posthog, pdfminer.six, onnxruntime, msoffcrypto-tool, langchainplus-sdk, httpx, gpt4all, fastapi, dataclasses-json, langchain, argilla, unstructured, chromadb, pcodedmp, oletools, RTFDE, extract-msg\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.0.7\n",
            "    Uninstalling urllib3-2.0.7:\n",
            "      Successfully uninstalled urllib3-2.0.7\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.66.2\n",
            "    Uninstalling tqdm-4.66.2:\n",
            "      Successfully uninstalled tqdm-4.66.2\n",
            "  Attempting uninstall: pyparsing\n",
            "    Found existing installation: pyparsing 3.1.1\n",
            "    Uninstalling pyparsing-3.1.1:\n",
            "      Successfully uninstalled pyparsing-3.1.1\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.6.1\n",
            "    Uninstalling pydantic-2.6.1:\n",
            "      Successfully uninstalled pydantic-2.6.1\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.25.2\n",
            "    Uninstalling numpy-1.25.2:\n",
            "      Successfully uninstalled numpy-1.25.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "chex 0.1.85 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed PyMuPDF-1.22.5 RTFDE-0.0.2 XlsxWriter-3.2.0 argilla-1.25.0 backoff-2.2.1 chromadb-0.3.26 clickhouse-connect-0.7.1 colorclass-2.2.2 coloredlogs-15.0.1 compressed-rtf-1.0.6 dataclasses-json-0.5.14 deprecated-1.2.14 diskcache-5.6.3 easygui-0.98.3 ebcdic-1.1.1 extract-msg-0.41.5 fastapi-0.110.0 filetype-1.2.0 gpt4all-1.0.3 h11-0.14.0 hnswlib-0.8.0 httpcore-1.0.4 httptools-0.6.1 httpx-0.26.0 humanfriendly-10.0 imapclient-2.3.1 langchain-0.0.228 langchainplus-sdk-0.0.20 lark-parser-0.12.0 llama-cpp-python-0.1.68 lz4-4.3.3 marshmallow-3.21.0 monotonic-1.6 msg-parser-1.2.0 msoffcrypto-tool-5.3.1 mypy-extensions-1.0.0 numpy-1.23.5 olefile-0.46 oletools-0.60.1 onnxruntime-1.17.1 openapi-schema-pydantic-1.2.4 overrides-7.7.0 pandoc-2.3 pcodedmp-1.2.6 pdf2image-1.17.0 pdfminer.six-20231228 plumbum-1.8.2 ply-3.11 posthog-3.4.2 pulsar-client-3.4.0 pydantic-1.10.14 pypandoc-1.11 pyparsing-2.4.7 python-docx-1.1.0 python-dotenv-1.0.0 python-magic-0.4.27 python-pptx-0.6.23 red-black-tree-mod-1.20 starlette-0.36.3 tqdm-4.65.0 typing-inspect-0.9.0 unstructured-0.8.0 urllib3-2.0.3 uvicorn-0.27.1 uvloop-0.19.0 watchfiles-0.21.0 websockets-12.0 zstandard-0.22.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "aa920021a2b644c98b533e310f860bd4",
              "pip_warning": {
                "packages": [
                  "numpy",
                  "pyparsing"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting sentence_transformers\n",
            "  Downloading sentence_transformers-2.4.0-py3-none-any.whl (149 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/149.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m143.4/149.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.5/149.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.32.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.37.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.65.0)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.1.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.11.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.20.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (4.9.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence_transformers) (23.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (0.4.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2.0.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence_transformers) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
            "Installing collected packages: sentence_transformers\n",
            "Successfully installed sentence_transformers-2.4.0\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.65.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.9.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.2.2)\n",
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.2.53.tar.gz (36.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.8/36.8 MB\u001b[0m \u001b[31m220.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting typing-extensions>=4.5.0 (from llama-cpp-python)\n",
            "  Downloading typing_extensions-4.10.0-py3-none-any.whl (33 kB)\n",
            "Collecting numpy>=1.20.0 (from llama-cpp-python)\n",
            "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m238.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m169.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jinja2>=2.11.3 (from llama-cpp-python)\n",
            "  Downloading Jinja2-3.1.3-py3-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.2/133.2 kB\u001b[0m \u001b[31m222.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting MarkupSafe>=2.0 (from jinja2>=2.11.3->llama-cpp-python)\n",
            "  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Building wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hcanceled\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install langchain==0.0.228\n",
        "!pip install gpt4all==1.0.3\n",
        "!pip install chromadb==0.3.26\n",
        "!pip install llama-cpp-python==0.1.68\n",
        "!pip install urllib3==2.0.3\n",
        "!pip install PyMuPDF==1.22.5\n",
        "!pip install python-dotenv==1.0.0\n",
        "!pip install unstructured==0.8.0\n",
        "!pip install extract-msg==0.41.5\n",
        "!pip install tabulate==0.9.0\n",
        "!pip install pandoc==2.3\n",
        "!pip install pypandoc==1.11\n",
        "!pip install tqdm==4.65.0\n",
        "!pip install sentence_transformers\n",
        "!pip install huggingface_hub\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python --force-reinstall --upgrade --no-cache-dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcsRDmgseAeF"
      },
      "source": [
        "# Download the Model (Nous-Hermes-Llama2-GGML)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "4IlmSlKuAJBs",
        "outputId": "07316b5b-be2c-40e1-c063-59f6f49c06a9"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/privateGPT/models/nous-hermes-llama2-13b.ggmlv3.q4_1.bin'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "hf_hub_download(repo_id=\"TheBloke/Nous-Hermes-Llama2-GGML\", filename=\"nous-hermes-llama2-13b.ggmlv3.q4_1.bin\", local_dir=\"/content/models/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZGmSkyjeleS"
      },
      "source": [
        "# Ingest your documents and create a database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WncT6FB5EV5a",
        "outputId": "73bfc4c0-557e-40f3-8d95-cd225d4c072c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  return self.fget.__get__(instance, owner)()\n",
            "Creating new vectorstore\n",
            "Loading documents from /content/privateGPT/source_documents/\n",
            "Loading new documents: 100%|██████████████████████| 1/1 [00:00<00:00,  8.29it/s]\n",
            "Loaded 583 new documents from /content/privateGPT/source_documents/\n",
            "Split into 4696 chunks of text (max. 500 tokens each)\n",
            "Creating embeddings. May take some minutes...\n",
            "Ingestion complete! You can now run privateGPT.py to query your documents\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import glob\n",
        "from typing import List\n",
        "from dotenv import load_dotenv\n",
        "from multiprocessing import Pool\n",
        "from tqdm import tqdm\n",
        "\n",
        "from langchain.document_loaders import (\n",
        "    CSVLoader,\n",
        "    EverNoteLoader,\n",
        "    PyMuPDFLoader,\n",
        "    TextLoader,\n",
        "    UnstructuredEmailLoader,\n",
        "    UnstructuredEPubLoader,\n",
        "    UnstructuredHTMLLoader,\n",
        "    UnstructuredMarkdownLoader,\n",
        "    UnstructuredODTLoader,\n",
        "    UnstructuredPowerPointLoader,\n",
        "    UnstructuredWordDocumentLoader,\n",
        ")\n",
        "\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from chromadb.config import Settings\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "# Define the folder for storing database\n",
        "PERSIST_DIRECTORY = os.environ.get('PERSIST_DIRECTORY')\n",
        "\n",
        "# Define the Chroma settings\n",
        "CHROMA_SETTINGS = Settings(\n",
        "        chroma_db_impl='duckdb+parquet',\n",
        "        persist_directory=PERSIST_DIRECTORY,\n",
        "        anonymized_telemetry=False\n",
        ")\n",
        "\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.docstore.document import Document\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "\n",
        "# Load environment variables\n",
        "persist_directory = os.environ.get('PERSIST_DIRECTORY')\n",
        "source_directory = \"/content/source_documents/\" # os.environ.get('SOURCE_DIRECTORY', 'source_documents')\n",
        "embeddings_model_name = os.environ.get('EMBEDDINGS_MODEL_NAME')\n",
        "chunk_size = 500\n",
        "chunk_overlap = 50\n",
        "\n",
        "\n",
        "# Custom document loaders\n",
        "class MyElmLoader(UnstructuredEmailLoader):\n",
        "    \"\"\"Wrapper to fallback to text/plain when default does not work\"\"\"\n",
        "\n",
        "    def load(self) -> List[Document]:\n",
        "        \"\"\"Wrapper adding fallback for elm without html\"\"\"\n",
        "        try:\n",
        "            try:\n",
        "                doc = UnstructuredEmailLoader.load(self)\n",
        "            except ValueError as e:\n",
        "                if 'text/html content not found in email' in str(e):\n",
        "                    # Try plain text\n",
        "                    self.unstructured_kwargs[\"content_source\"]=\"text/plain\"\n",
        "                    doc = UnstructuredEmailLoader.load(self)\n",
        "                else:\n",
        "                    raise\n",
        "        except Exception as e:\n",
        "            # Add file_path to exception message\n",
        "            raise type(e)(f\"{self.file_path}: {e}\") from e\n",
        "\n",
        "        return doc\n",
        "\n",
        "\n",
        "# Map file extensions to document loaders and their arguments\n",
        "LOADER_MAPPING = {\n",
        "    \".csv\": (CSVLoader, {}),\n",
        "    # \".docx\": (Docx2txtLoader, {}),\n",
        "    \".doc\": (UnstructuredWordDocumentLoader, {}),\n",
        "    \".docx\": (UnstructuredWordDocumentLoader, {}),\n",
        "    \".enex\": (EverNoteLoader, {}),\n",
        "    \".eml\": (MyElmLoader, {}),\n",
        "    \".epub\": (UnstructuredEPubLoader, {}),\n",
        "    \".html\": (UnstructuredHTMLLoader, {}),\n",
        "    \".md\": (UnstructuredMarkdownLoader, {}),\n",
        "    \".odt\": (UnstructuredODTLoader, {}),\n",
        "    \".pdf\": (PyMuPDFLoader, {}),\n",
        "    \".ppt\": (UnstructuredPowerPointLoader, {}),\n",
        "    \".pptx\": (UnstructuredPowerPointLoader, {}),\n",
        "    \".txt\": (TextLoader, {\"encoding\": \"utf8\"}),\n",
        "    # Add more mappings for other file extensions and loaders as needed\n",
        "}\n",
        "\n",
        "\n",
        "def load_single_document(file_path: str) -> List[Document]:\n",
        "    ext = \".\" + file_path.rsplit(\".\", 1)[-1]\n",
        "    if ext in LOADER_MAPPING:\n",
        "        loader_class, loader_args = LOADER_MAPPING[ext]\n",
        "        loader = loader_class(file_path, **loader_args)\n",
        "        return loader.load()\n",
        "\n",
        "    raise ValueError(f\"Unsupported file extension '{ext}'\")\n",
        "\n",
        "def load_documents(source_dir: str, ignored_files: List[str] = []) -> List[Document]:\n",
        "    \"\"\"\n",
        "    Loads all documents from the source documents directory, ignoring specified files\n",
        "    \"\"\"\n",
        "    all_files = []\n",
        "    for ext in LOADER_MAPPING:\n",
        "        all_files.extend(\n",
        "            glob.glob(os.path.join(source_dir, f\"**/*{ext}\"), recursive=True)\n",
        "        )\n",
        "    filtered_files = [file_path for file_path in all_files if file_path not in ignored_files]\n",
        "\n",
        "    with Pool(processes=os.cpu_count()) as pool:\n",
        "        results = []\n",
        "        with tqdm(total=len(filtered_files), desc='Loading new documents', ncols=80) as pbar:\n",
        "            for i, docs in enumerate(pool.imap_unordered(load_single_document, filtered_files)):\n",
        "                results.extend(docs)\n",
        "                pbar.update()\n",
        "\n",
        "    return results\n",
        "\n",
        "def process_documents(ignored_files: List[str] = []) -> List[Document]:\n",
        "    \"\"\"\n",
        "    Load documents and split in chunks\n",
        "    \"\"\"\n",
        "    print(f\"Loading documents from {source_directory}\")\n",
        "    documents = load_documents(source_directory, ignored_files)\n",
        "    if not documents:\n",
        "        print(\"No new documents to load\")\n",
        "        exit(0)\n",
        "    print(f\"Loaded {len(documents)} new documents from {source_directory}\")\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "    texts = text_splitter.split_documents(documents)\n",
        "    print(f\"Split into {len(texts)} chunks of text (max. {chunk_size} tokens each)\")\n",
        "    return texts\n",
        "\n",
        "def does_vectorstore_exist(persist_directory: str) -> bool:\n",
        "    \"\"\"\n",
        "    Checks if vectorstore exists\n",
        "    \"\"\"\n",
        "    if os.path.exists(os.path.join(persist_directory, 'index')):\n",
        "        if os.path.exists(os.path.join(persist_directory, 'chroma-collections.parquet')) and os.path.exists(os.path.join(persist_directory, 'chroma-embeddings.parquet')):\n",
        "            list_index_files = glob.glob(os.path.join(persist_directory, 'index/*.bin'))\n",
        "            list_index_files += glob.glob(os.path.join(persist_directory, 'index/*.pkl'))\n",
        "            # At least 3 documents are needed in a working vectorstore\n",
        "            if len(list_index_files) > 3:\n",
        "                return True\n",
        "    return False\n",
        "\n",
        "def main():\n",
        "    # Create embeddings\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=embeddings_model_name)\n",
        "\n",
        "    if does_vectorstore_exist(persist_directory):\n",
        "        # Update and store locally vectorstore\n",
        "        print(f\"Appending to existing vectorstore at {persist_directory}\")\n",
        "        db = Chroma(persist_directory=persist_directory, embedding_function=embeddings, client_settings=CHROMA_SETTINGS)\n",
        "        collection = db.get()\n",
        "        texts = process_documents([metadata['source'] for metadata in collection['metadatas']])\n",
        "        print(f\"Creating embeddings. May take some minutes...\")\n",
        "        db.add_documents(texts)\n",
        "    else:\n",
        "        # Create and store locally vectorstore\n",
        "        print(\"Creating new vectorstore\")\n",
        "        texts = process_documents()\n",
        "        print(f\"Creating embeddings. May take some minutes...\")\n",
        "        db = Chroma.from_documents(texts, embeddings, persist_directory=persist_directory, client_settings=CHROMA_SETTINGS)\n",
        "    db.persist()\n",
        "    db = None\n",
        "\n",
        "    print(f\"Ingestion complete! You can now run privateGPT.py to query your documents\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ip5qB9F7XSu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwQGLChJe2bg"
      },
      "source": [
        "# Chat with your document (wait for \"Enter Query:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VEK2ukYPfnI",
        "outputId": "59847878-a081-4169-a654-1220f4685b4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  return self.fget.__get__(instance, owner)()\n",
            "llama.cpp: loading model from /content/privateGPT/models/nous-hermes-llama2-13b.ggmlv3.q4_1.bin\n",
            "llama_model_load_internal: format     = ggjt v3 (latest)\n",
            "llama_model_load_internal: n_vocab    = 32032\n",
            "llama_model_load_internal: n_ctx      = 2048\n",
            "llama_model_load_internal: n_embd     = 5120\n",
            "llama_model_load_internal: n_mult     = 256\n",
            "llama_model_load_internal: n_head     = 40\n",
            "llama_model_load_internal: n_layer    = 40\n",
            "llama_model_load_internal: n_rot      = 128\n",
            "llama_model_load_internal: ftype      = 3 (mostly Q4_1)\n",
            "llama_model_load_internal: n_ff       = 13824\n",
            "llama_model_load_internal: model size = 13B\n",
            "llama_model_load_internal: ggml ctx size =    0.09 MB\n",
            "llama_model_load_internal: mem required  = 9807.68 MB (+ 1608.00 MB per state)\n",
            "llama_new_context_with_model: kv self size  = 1600.00 MB\n",
            "\n",
            "Enter a query: Define a merger of reliance with walt Disney\n",
            " A merger refers to the combination of two or more existing organizations into a single new entity, where one or more of the previous organizations cease to exist. In this context, a merger between Reliance and Walt Disney would involve the combining of their respective businesses and assets, with the goal of creating a larger and more competitive company in the media and entertainment industry. The exact details and terms of such a merger would need to be worked out by both companies and their respective advisors.\n",
            "\n",
            "> Question:\n",
            "Define a merger of reliance with walt Disney\n",
            "\n",
            "> Answer (took 841.5 s.):\n",
            " A merger refers to the combination of two or more existing organizations into a single new entity, where one or more of the previous organizations cease to exist. In this context, a merger between Reliance and Walt Disney would involve the combining of their respective businesses and assets, with the goal of creating a larger and more competitive company in the media and entertainment industry. The exact details and terms of such a merger would need to be worked out by both companies and their respective advisors.\n",
            "\n",
            "> /content/privateGPT/source_documents/News.csv:\n",
            "\"We have always respected Disney as the best media group globally and are very excited at forming this strategic joint venture that will help us pool our extensive resources, creative prowess, and market insights to deliver unparalleled content at affordable prices to audiences across the nation. We welcome Disney as a key partner of Reliance group,\" he said.\n",
            "\n",
            "> /content/privateGPT/source_documents/News.csv:\n",
            "The joint venture will also be granted exclusive rights to distribute Disney films and productions in India, with a license to more than 30,000 Disney content assets, providing a full suite of entertainment options for the Indian consumer.\n",
            "\n",
            "Speaking about the merger, Mukesh D Ambani, Chairman & Managing Director of Reliance Industries, said, this is a landmark agreement that heralds a new era in the Indian entertainment industry.\n",
            "\n",
            "> /content/privateGPT/source_documents/News.csv:\n",
            ": 0\n",
            "Headline: Walt Disney, Reliance merge India media operations to create Rs 70,000 crore behemoth\n",
            "URL: https://www.deccanherald.com/business/walt-disney-reliance-merge-india-media-operations-to-create-rs-70000-crore-behemoth-2914334\n",
            "Content: Disney+Hotstar has seen its paid subscriber base decline from around 55 million to about 40 million in the first quarter of this financial year because of Reliance's Jio Cinemas winning exclusive rights for live sports.\n",
            "\n",
            "> /content/privateGPT/source_documents/News.csv:\n",
            ": 41\n",
            "Headline: Disney plans $8.5bn merger for ailing India unit\n",
            "URL: https://www.bbc.com/news/business-68427037\n",
            "Content: Disney plans $8.5bn merger for ailing India unit\n",
            "\n",
            "Getty Images\n",
            "\n",
            "Disney is combining forces with Asia's richest man in a bid to solve challenges weighing down its streaming business in India.\n",
            "\n",
            "The company said it was merging its Star India service with Viacom18, which is backed by Mukesh Ambani's Reliance Industries, in a deal worth $8.5bn.\n",
            "\n",
            "Enter a query: What is the document about?\n",
            " The document describes a study conducted on a group of people who consumed a cup of coffee before going to sleep. It reports that the participants had an easier time falling asleep and slept longer than usual.\n",
            "\n",
            "> Question:\n",
            "What is the document about?\n",
            "\n",
            "> Answer (took 631.3 s.):\n",
            " The document describes a study conducted on a group of people who consumed a cup of coffee before going to sleep. It reports that the participants had an easier time falling asleep and slept longer than usual.\n",
            "\n",
            "> /content/privateGPT/source_documents/News.csv:\n",
            "witness in the trial. It is an \"exuberant\" novel, says The Observer, and bears the author's usual trademarks: \"the boisterous narrative intelligence; the ear for dialogue; the chronic absence of boring sentences\". There is also a lightness of touch: \"Every few pages I was struck by how light the novel feels, despite its length and epic themes. The short chapters glide tellingly between decades and scenes.\" The Conversation describes The Fraud as \"a stunning, well-studied examination of\n",
            "\n",
            "> /content/privateGPT/source_documents/News.csv:\n",
            "inextricably, into the devastating history of the Holocaust in France\". The novel is, says Library Journal, \"not only a significant contribution to our understanding of the Holocaust but a moving reflection on loss, memory, and the past, in equal measures heart-warming and heartrending\". The Washington Post describes The Postcard as \"a powerful exploration of family trauma... transmitted in the womb or down the generations.\" (LB)\n",
            "\n",
            "> /content/privateGPT/source_documents/News.csv:\n",
            ": 352\n",
            "Headline: How Amazon MGM Studios is writing the playbook for representation – on-screen and off\n",
            "URL: https://www.bbc.com/worklife/article/20240222-bbc-interview-latasha-gillespie-amazon-mgm-studios\n",
            "Content: How Amazon MGM Studios is writing the playbook for representation – on-screen and off\n",
            "\n",
            "By Kieron Johnson Features correspondent\n",
            "\n",
            "Klawe Rzeczy\n",
            "\n",
            "> /content/privateGPT/source_documents/News.csv:\n",
            "– and are forced to reconsider everything they thought they knew. \"Tom Lake is a fascinating story beautifully told,\" says the New York Journal of Books, and the novel's structure \"is wonderfully measured, as Patchett weaves the fine details of dual timelines together\". The Washington Post praisesthe book's \"remarkable warmth\", and Pratchett's \"wisdom about love\" – Tom Lake, it says, \"reminds us why she's beloved\". (LB)\n",
            "\n",
            "Enter a query: "
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "from dotenv import load_dotenv\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.llms import GPT4All, LlamaCpp\n",
        "import os\n",
        "import argparse\n",
        "import time\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "embeddings_model_name = os.environ.get(\"EMBEDDINGS_MODEL_NAME\")\n",
        "persist_directory = os.environ.get('PERSIST_DIRECTORY')\n",
        "\n",
        "model_type = os.environ.get('MODEL_TYPE')\n",
        "model_path = os.environ.get('MODEL_PATH')\n",
        "model_n_ctx = os.environ.get('MODEL_N_CTX')\n",
        "model_n_batch = int(os.environ.get('MODEL_N_BATCH',8))\n",
        "target_source_chunks = int(os.environ.get('TARGET_SOURCE_CHUNKS',4))\n",
        "\n",
        "def main():\n",
        "    # Parse the command line arguments\n",
        "    args = parse_arguments()\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=embeddings_model_name)\n",
        "    db = Chroma(persist_directory=persist_directory, embedding_function=embeddings, client_settings=CHROMA_SETTINGS)\n",
        "    retriever = db.as_retriever(search_kwargs={\"k\": target_source_chunks})\n",
        "    # activate/deactivate the streaming StdOut callback for LLMs\n",
        "    callbacks = [] if args.mute_stream else [StreamingStdOutCallbackHandler()]\n",
        "    # Prepare the LLM\n",
        "    match model_type:\n",
        "        case \"LlamaCpp\":\n",
        "            llm = LlamaCpp(model_path=model_path, max_tokens=model_n_ctx, n_batch=model_n_batch, callbacks=callbacks, verbose=False, n_gpu_layers=40, n_ctx=2048)\n",
        "        case \"GPT4All\":\n",
        "            llm = GPT4All(model=model_path, max_tokens=model_n_ctx, backend='gptj', n_batch=model_n_batch, callbacks=callbacks, verbose=False)\n",
        "        case _default:\n",
        "            # raise exception if model_type is not supported\n",
        "            raise Exception(f\"Model type {model_type} is not supported. Please choose one of the following: LlamaCpp, GPT4All\")\n",
        "\n",
        "    qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever, return_source_documents= not args.hide_source)\n",
        "    # Interactive questions and answers\n",
        "    while True:\n",
        "        query = input(\"\\nEnter a query: \")\n",
        "        if query == \"exit\":\n",
        "            break\n",
        "        if query.strip() == \"\":\n",
        "            continue\n",
        "\n",
        "        # Get the answer from the chain\n",
        "        start = time.time()\n",
        "        res = qa(query)\n",
        "        answer, docs = res['result'], [] if args.hide_source else res['source_documents']\n",
        "        end = time.time()\n",
        "\n",
        "        # Print the result\n",
        "        print(\"\\n\\n> Question:\")\n",
        "        print(query)\n",
        "        print(f\"\\n> Answer (took {round(end - start, 2)} s.):\")\n",
        "        print(answer)\n",
        "\n",
        "        # Print the relevant sources used for the answer\n",
        "        for document in docs:\n",
        "            print(\"\\n> \" + document.metadata[\"source\"] + \":\")\n",
        "            print(document.page_content)\n",
        "\n",
        "def parse_arguments():\n",
        "    parser = argparse.ArgumentParser(description='privateGPT: Ask questions to your documents without an internet connection, '\n",
        "                                                 'using the power of LLMs.')\n",
        "    parser.add_argument(\"--hide-source\", \"-S\", action='store_true',\n",
        "                        help='Use this flag to disable printing of source documents used for answers.')\n",
        "\n",
        "    parser.add_argument(\"--mute-stream\", \"-M\",\n",
        "                        action='store_true',\n",
        "                        help='Use this flag to disable the streaming StdOut callback for LLMs.')\n",
        "\n",
        "    return parser.parse_args()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GymSliVD-b8F"
      },
      "source": [
        "# Maintenance (*Optional*)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-7Xk4Cof-VsZ"
      },
      "outputs": [],
      "source": [
        "!rm -rf /content/privateGPT"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
